{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47d9dc2",
   "metadata": {},
   "source": [
    "# FACIAL EMOTION CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f962c5",
   "metadata": {},
   "source": [
    "## Step 1: Problem Identification and Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d2cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc69b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "088fc8f3",
   "metadata": {},
   "source": [
    "## Step - 2 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c0ef1",
   "metadata": {},
   "source": [
    "## For scraping images to create our dataset we have used 2 sources -\n",
    "1. Google Image search\n",
    "2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1759121",
   "metadata": {},
   "source": [
    "Google image seach was built using chromedriver and follows these steps -\n",
    "1. Open chrome windows\n",
    "2. Search google images with the provided query string\n",
    "3. Click on each image in the results\n",
    "4. Get the link for the the images\n",
    "5. After all links are collected for a query, the images are downloaded and saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69030c",
   "metadata": {},
   "source": [
    "Instagram hastag approach followed these steps -\n",
    "1.\n",
    "2.\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f9b47",
   "metadata": {},
   "source": [
    "#### Scraping data from google images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5057434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import hashlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7461d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)    \n",
    "    \n",
    "    # build the google query\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        number_results = len(thumbnail_results)\n",
    "             \n",
    "        for img in thumbnail_results[results_start:number_results]:\n",
    "            # try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # extract image urls    \n",
    "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "            image_count = len(image_urls)\n",
    "\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                break\n",
    "        else:\n",
    "            time.sleep(30)\n",
    "            return\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = len(thumbnail_results)\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ef40323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(folder_path:str,file_name:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        folder_path = os.path.join(folder_path,file_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        else:\n",
    "            os.mkdir(folder_path)\n",
    "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30654392",
   "metadata": {},
   "source": [
    "#### Chromedriver\n",
    "- chromedriver.exe is used to open a chrome browser window before can start scraping data.\n",
    "- The proper chrome driver can be download from : https://chromedriver.chromium.org/downloads\n",
    "- We have included the chromedriver for windows with our repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3682ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path for chrome driver\n",
    "CHROME_DRIVER_PATH = os.path.abspath('./') + \"\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29a398",
   "metadata": {},
   "source": [
    "#### Downloading images for category : \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e40c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queries = [\"happy human face\", \"happy black man face\", \"happy white man face\", \"happy black woman face\", \"happy white woman face\", \"happy asian face\", \"happy indian man face\", \"happy indian woman face\"]\n",
    "for query in queries:\n",
    "    wd = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "    wd.get('https://google.com')\n",
    "    search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "    search_box.send_keys(query)\n",
    "    links = fetch_image_urls(query,100,wd)\n",
    "    images_path = os.path.abspath('./') + '/downloaded_data'\n",
    "    if not os.path.exists(images_path):\n",
    "        os.mkdir(images_path)\n",
    "    for i in links:\n",
    "        download_image(images_path,query,i)\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c781ef",
   "metadata": {},
   "source": [
    "#### Downloading images for category : \"sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3ea4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queries = [\"sad human face\", \"sad black man face\", \"sad white man face\", \"sad black woman face\", \"sad white woman face\", \"sad asian face\", \"sad indian man face\", \"sad indian woman face\"]\n",
    "for query in queries:\n",
    "    wd = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "    wd.get('https://google.com')\n",
    "    search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "    search_box.send_keys(query)\n",
    "    links = fetch_image_urls(query,100,wd)\n",
    "    images_path = os.path.abspath('./') + '/downloaded_data'\n",
    "    if not os.path.exists(images_path):\n",
    "        os.mkdir(images_path)\n",
    "    for i in links:\n",
    "        download_image(images_path,query,i)\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ef5a1",
   "metadata": {},
   "source": [
    "#### Downloading images for category : \"angry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a618c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queries = [\"angry human face\", \"angry black man face\", \"angry white man face\", \"angry black woman face\", \"angry white woman face\", \"angry asian face\", \"angry indian man face\", \"angry indian woman face\"]\n",
    "for query in queries:\n",
    "    wd = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "    wd.get('https://google.com')\n",
    "    search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "    search_box.send_keys(query)\n",
    "    links = fetch_image_urls(query,100,wd)\n",
    "    images_path = os.path.abspath('./') + '/downloaded_data'\n",
    "    for i in links:\n",
    "        download_image(images_path,query,i)\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c710fe3",
   "metadata": {},
   "source": [
    "#### Downloading images for category : \"disgusted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2621c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queries = [\"disgusted human face\", \"disgusted black man face\", \"disgusted white man face\", \"disgusted black woman face\", \"disgusted white woman face\", \"disgusted asian face\", \"disgusted indian man face\", \"disgusted indian woman face\"]\n",
    "for query in queries:\n",
    "    wd = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "    wd.get('https://google.com')\n",
    "    search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "    search_box.send_keys(query)\n",
    "    links = fetch_image_urls(query,100,wd)\n",
    "    images_path = os.path.abspath('./') + '/downloaded_data'\n",
    "    if not os.path.exists(images_path):\n",
    "        os.mkdir(images_path)\n",
    "    for i in links:\n",
    "        download_image(images_path,query,i)\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba0770",
   "metadata": {},
   "source": [
    "#### Downloading images for category : \"surprised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4abb5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queries = [\"surprised black man face\", \"surprised white man face\", \"surprised black woman face\", \"surprised white woman face\", \"surprised indian man face\", \"surprised indian woman face\", \"surprised human face\",\"surprised asian face\"]\n",
    "for query in queries:\n",
    "    wd = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "    wd.get('https://google.com')\n",
    "    search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "    search_box.send_keys(query)\n",
    "    links = fetch_image_urls(query,100,wd)\n",
    "    images_path = os.path.abspath('./') + '/downloaded_data'\n",
    "    if not os.path.exists(images_path):\n",
    "        os.mkdir(images_path)\n",
    "    for i in links:\n",
    "        download_image(images_path,query,i)\n",
    "    wd.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8218d4a",
   "metadata": {},
   "source": [
    "#### Scraping data from instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14b3d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mxnet gluoncv\n",
    "import mxnet as mx\n",
    "from gluoncv import model_zoo, data, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab013dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class YoloV3:\n",
    "    def __init__(self):\n",
    "        \"\"\"Yolo V3 pretrained model\n",
    "           VOC dataset: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor\n",
    "        \"\"\"\n",
    "        self.net = model_zoo.get_model('yolo3_darknet53_voc', pretrained=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(image):\n",
    "        \"\"\"Load and transform the image\n",
    "        \"\"\"\n",
    "        return data.transforms.presets.yolo.transform_test(mx.ndarray.array(image), short=512)\n",
    "\n",
    "    def get_object_labels(self, class_ids, scores, score_th=0.5):\n",
    "        \"\"\"Filter detections by score and obtain the corresponding label for each class\n",
    "        \"\"\"\n",
    "        class_ids = class_ids[0].asnumpy().reshape(1, -1)\n",
    "        scores = scores[0].asnumpy().reshape(1, -1)\n",
    "        selected_class_ids = class_ids[scores > score_th]\n",
    "        return [self.net.classes[int(class_id)] for class_id in selected_class_ids]\n",
    "\n",
    "    def detect_objects(self, image):\n",
    "        \"\"\"Object detection in an image\n",
    "        \"\"\"\n",
    "        x, img = self.preprocess(image)\n",
    "        class_ids, scores, bboxs = self.net(x)\n",
    "        return class_ids, scores, bboxs\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"1. Detect objects\n",
    "           2. Get object labels. For this task we are only interested in what elements are in the image, we don't need more information\n",
    "        \"\"\"\n",
    "        image = kwargs.get('image')\n",
    "        class_ids, scores, _ = self.detect_objects(image)\n",
    "        labels = self.get_object_labels(class_ids, scores)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "762b260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install webdriver_manager.chrome selenium\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e20a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class InstaBot:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.user = 'Enter username for instagram'\n",
    "        self.password = 'Enter password for instagram'\n",
    "        self.path_out = os.path.abspath('./') + '/downloaded_data'\n",
    "\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument('--incognito')\n",
    "        # self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        self.driver = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH)\n",
    "        self.yolov3 = YoloV3()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image(url_image):\n",
    "        \"\"\"Download image from url image\n",
    "        \"\"\"\n",
    "        image_data = requests.get(url_image)\n",
    "        image_data_content = image_data.content\n",
    "        return Image.open(BytesIO(image_data_content))\n",
    "\n",
    "    def save_image(self, image, label):\n",
    "        \"\"\"Save image in the folder indicated by the label\n",
    "        \"\"\"\n",
    "        out_folder_path = os.path.join(self.path_out, \"happy\")\n",
    "        os.makedirs(out_folder_path, exist_ok=True)\n",
    "        out_image_path = os.path.join(out_folder_path, '{}.jpg'.format(str(time.time())))\n",
    "        image.save(out_image_path)\n",
    "\n",
    "    def labels_in_image(self, image):\n",
    "        \"\"\"Get object labels in the image using Yolo V3\n",
    "        \"\"\"\n",
    "        return list(set(self.yolov3(image=image)))\n",
    "\n",
    "    def get_data_images(self):\n",
    "        \"\"\"Get data images from html.\n",
    "        \"\"\"\n",
    "        html_to_parse = str(self.driver.page_source)\n",
    "        html = bs(html_to_parse, 'html.parser')\n",
    "        return html.findAll('img', {'class': 'FFVAD'})\n",
    "\n",
    "    def download_images(self):\n",
    "        \"\"\"Main process to find, filter and download the images.\n",
    "           Obtaining the images by scrolling and using the YoloV3 to check if it includes what we are looking for.\n",
    "        \"\"\"\n",
    "        downloaded_images = []\n",
    "        r_scroll_h = 'return document.body.scrollHeight'\n",
    "        scroll_h = 'window.scrollTo(0, document.body.scrollHeight);'\n",
    "\n",
    "        lh = self.driver.execute_script(r_scroll_h)\n",
    "        while True:\n",
    "            self.driver.execute_script(scroll_h)\n",
    "            self.driver.implicitly_wait(1)\n",
    "            nh = self.driver.execute_script(r_scroll_h)\n",
    "\n",
    "            if nh == lh:\n",
    "                self.driver.execute_script(scroll_h)\n",
    "                continue\n",
    "            else:\n",
    "                lh = nh\n",
    "                self.driver.implicitly_wait(1)\n",
    "\n",
    "            all_images_data = self.get_data_images()\n",
    "            non_downloaded_images = list(set(all_images_data) - set(downloaded_images))\n",
    "            for image_data in non_downloaded_images:\n",
    "                try:\n",
    "                    image = self.get_image(image_data.attrs['src'])\n",
    "                    labels = self.labels_in_image(image)\n",
    "                    for label in labels:\n",
    "                        if label in ['person']:\n",
    "                            self.save_image(image, label)\n",
    "                            downloaded_images.append(image_data)\n",
    "                except Exception:\n",
    "                    logging.warning('Error downloading an image')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.driver.get('https://instagram.com')\n",
    "        self.driver.implicitly_wait(2)\n",
    "\n",
    "        self.driver.find_element_by_xpath('//input[@name=\\\"username\\\"]').send_keys(self.user)\n",
    "        self.driver.find_element_by_xpath('//input[@name=\\\"password\\\"]').send_keys(self.password)\n",
    "        self.driver.find_element_by_xpath('//button[@type=\\\"submit\\\"]').click()\n",
    "        self.driver.implicitly_wait(4)\n",
    "\n",
    "        self.driver.find_element_by_xpath('//button[contains(text(), \"Not Now\")]').click()\n",
    "        self.driver.implicitly_wait(4)\n",
    "\n",
    "        self.driver.find_element_by_xpath('//input[@type=\\\"text\\\"]').send_keys('#disgustedselfie')\n",
    "        time.sleep(3)\n",
    "        for _ in range(2):\n",
    "            self.driver.find_element_by_xpath('//input[@type=\\\"text\\\"]').send_keys(Keys.ENTER)\n",
    "\n",
    "        self.download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677475b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = InstaBot()\n",
    "bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c198248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0491c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0ea52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "155d9ebf",
   "metadata": {},
   "source": [
    "## Step 3: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062af45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b7d41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data_path = os.path.abspath('./') + '/downloaded_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e3506",
   "metadata": {},
   "source": [
    "### Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0e6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path = downloaded_data_path + \"/happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5d8391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1632977270.4202986.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([x for x in os.listdir(happy_path) if os.path.isfile(os.path.join(happy_path, x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b53db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571ea48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719facb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449d5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56974148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44b7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446e836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14df2c5",
   "metadata": {},
   "source": [
    "## Step 4: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6650c30",
   "metadata": {},
   "source": [
    "### Cropping and face centering image data\n",
    "\n",
    "- The images downlaod from instagram and google images had a background apart from the face.\n",
    "- For training we only need the facial data\n",
    "- So, we used a pre-trained deeplearning network MTCNN to detect faces in the data and cropped out the faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55832e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c70e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceCropper(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.detector = MTCNN()\n",
    "    \n",
    "    def generate(self, image_path,prefix,output_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        if (img is None):\n",
    "            return 0\n",
    "        faces = self.detector.detect_faces(img)\n",
    "        for i, face in enumerate(faces): # iterate through all the faces found\n",
    "            box=face['box']\n",
    "            if box !=[]:\n",
    "                confidence = face['confidence']\n",
    "                if(confidence < 0.9):\n",
    "                    continue\n",
    "                # return all faces found in the image\n",
    "                box[0]= 0 if box[0]<0 else box[0]\n",
    "                box[1]= 0 if box[1]<0 else box[1]\n",
    "                \n",
    "                cropped_img=img[box[1]: box[1]+box[3],box[0]: box[0]+ box[2]]\n",
    "                try:\n",
    "                    cv2.imwrite(output_data_path + prefix, cropped_img)\n",
    "                except Exception as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc7a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "detecter = FaceCropper()\n",
    "categories = [\"happy\", \"sad\", \"angry\", \"disgusted\",\"surprised\"]\n",
    "data_folder = \"/downloaded_data/\"\n",
    "for category in categories:\n",
    "    input_data_path = os.path.abspath('./') + data_folder + category\n",
    "    output_data_path = os.path.abspath('./') + \"/final_data\"\n",
    "\n",
    "    if not os.path.exists(output_data_path):\n",
    "        os.mkdir(output_data_path)\n",
    "    output_data_path = output_data_path + \"/\" + category + \"/\"\n",
    "    if not os.path.exists(output_data_path):\n",
    "        os.mkdir(output_data_path)\n",
    "    for filename in os.listdir(input_data_path):\n",
    "        image_path = input_data_path + \"/\" + filename\n",
    "        detecter.generate(image_path,filename,output_data_path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ee615",
   "metadata": {},
   "source": [
    "### Rename data files\n",
    "\n",
    "- We renamed the data files to ease the process of label creation.\n",
    "- The files were renamed in the following method -- category_1, category_2 ..\n",
    "- For examples - happy_0, happy_1 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943aabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b320af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(data_path,prefix):\n",
    "    for count, f in enumerate(os.listdir(data_path)):\n",
    "        f_name, f_ext = os.path.splitext(f)\n",
    "        f_name = prefix + \"_\" +str(count)\n",
    " \n",
    "        new_name = f'{f_name}{f_ext}'\n",
    "        os.rename(data_path + f, data_path + new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4a014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sad\", \"happy\", \"surprised\", \"angry\", \"disgusted\"]\n",
    "for category in categories:\n",
    "    data_folder = category\n",
    "    data_path = os.path.abspath('./') + \"/\" + \"/final_data/\" + data_folder + \"/\"\n",
    "    rename_files(data_path,data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab21a1a",
   "metadata": {},
   "source": [
    "### Split the data into train and test set\n",
    "Splitting of data into train and test set in the ratio 70:30.\n",
    "The data across the 5 categories has been split in stratified manner thus ensuring the same percentage of a category in the train and test set as was in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea94989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import random \n",
    "import shutil, random, os\n",
    "from torchvision import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d287edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {'angry':240, 'disgusted':224, 'happy':528, 'sad':384, 'surprised':129}\n",
    "\n",
    "img_folder = os.path.abspath('./') + \"/final_data/\"\n",
    "testdestDirectory = os.path.abspath('./')+ \"/split_data/\" +'/test/'\n",
    "traindestDirectory = os.path.abspath('./')+ \"/split_data/\" +'/train/'\n",
    "\n",
    "for categories in os.listdir(img_folder):\n",
    "    if categories in l:\n",
    "        files = [file for file in os.listdir(os.path.join(img_folder, categories))]\n",
    "        random_files = random.sample(files, l[categories])\n",
    "        rem = list(set(files)-set(random_files))\n",
    "        for fname in random_files:\n",
    "            srcpath = os.path.join(img_folder, categories, fname)\n",
    "            tsdestDirectory = os.path.join(testdestDirectory, categories)\n",
    "            if not os.path.exists(tsdestDirectory):\n",
    "                os.makedirs(tsdestDirectory)\n",
    "            tsdestDirectory = os.path.join(testdestDirectory, categories,fname)\n",
    "            shutil.copyfile(srcpath, tsdestDirectory)\n",
    "        for name in rem:\n",
    "            srcpath = os.path.join(img_folder, categories, name)\n",
    "            trdestDirectory = os.path.join(traindestDirectory, categories)\n",
    "            if not os.path.exists(trdestDirectory):\n",
    "                os.makedirs(trdestDirectory)\n",
    "            trdestDirectory = os.path.join(traindestDirectory, categories,name)\n",
    "            shutil.copyfile(srcpath, trdestDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1bbc4",
   "metadata": {},
   "source": [
    "### Create dataloader for the image files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552d1da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34ebf570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_loader(data_dir, img_size, batch_size):\n",
    "    data_transforms = T.Compose([\n",
    "                                T.RandomRotation(30),\n",
    "                                T.RandomHorizontalFlip(),\n",
    "                                T.Resize(img_size),\n",
    "                                T.ToTensor()])\n",
    "    data = datasets.ImageFolder(data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(data,batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a987830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.abspath('./')+ \"/split_data/\" +'/train/'\n",
    "test_dir = os.path.abspath('./')+ \"/split_data/\" + '/test/'\n",
    "\n",
    "train_data_loader = get_dataset_loader(train_dir, (64,64),32)\n",
    "test_data_loader = get_dataset_loader(test_dir, (64,64),32)\n",
    "\n",
    "train_data_generator = iter(train_data_loader)\n",
    "test_data_generator = iter(test_data_loader)\n",
    "\n",
    "#How to access the images and labels\n",
    "images,labels = next(train_data_generator)\n",
    "images,labels = next(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd3aed93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 4, 0, 3, 3, 2, 3, 1, 2, 2, 2, 3, 3, 4, 2, 2, 4, 0, 3, 2, 4, 2, 4,\n",
       "        2, 0, 0, 3, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25190bb",
   "metadata": {},
   "source": [
    "## Step 5: Data analysis with deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61b2ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "848b8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20045f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f58e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e3ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaf4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16888ad1",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e307fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648ef1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58af4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1699f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50998d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "606ab3c1",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863f82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb5f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47eca99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c942ec9",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae914b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1586b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf02d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d6387e1",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926d9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72480dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04502b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f8dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3e802e7",
   "metadata": {},
   "source": [
    "## Step 6: Numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c5747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e121974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130a6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1111c429",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1291c",
   "metadata": {},
   "source": [
    "1. https://medium.com/@wwwanandsuresh/web-scraping-images-from-google-9084545808a2\n",
    "2. https://gist.github.com/tilfin/98bbba47fdc4ac10c4069cce5fabd834\n",
    "3. https://github.com/stanford-traffic-sign/stanford-cs230-traffic-sign\n",
    "4. https://chromedriver.chromium.org/downloads\n",
    "5. https://www.geeksforgeeks.org/rename-all-file-names-in-your-directory-using-python/\n",
    "6. https://github.com/dheeraj-sn/facedetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c8aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
